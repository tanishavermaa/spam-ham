{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46d5207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hrisi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Date:        Wed, 21 Aug 2002 10:54:46 -05...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Martin A posted:\\r\\n\\r\\nTassos Papadopoulos, t...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Man Threatens Explosion In Moscow \\r\\n\\r\\n\\r\\n...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Klez: The Virus That Won't Die\\r\\n\\r\\n \\r\\n\\r\\...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&gt;  in adding cream to spaghetti carbonara, whi...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message class\n",
       "0      Date:        Wed, 21 Aug 2002 10:54:46 -05...   ham\n",
       "1  Martin A posted:\\r\\n\\r\\nTassos Papadopoulos, t...   ham\n",
       "2  Man Threatens Explosion In Moscow \\r\\n\\r\\n\\r\\n...   ham\n",
       "3  Klez: The Virus That Won't Die\\r\\n\\r\\n \\r\\n\\r\\...   ham\n",
       "4  >  in adding cream to spaghetti carbonara, whi...   ham"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "messages = pd.read_csv(\"spamham.csv\", usecols=['message', 'class'])\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bcdaba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36df6a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81560ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = messages['class'].map({'ham':0, 'spam':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaeba00",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "- Why do we use Grid Search?\n",
    "\n",
    "`GridSearchCV` is a technique to search through the best parameter values from the given set of the grid of parameters. It is basically a cross-validation method. the model and the parameters are required to be fed in. Best parameter values are extracted and then the predictions are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85158b53",
   "metadata": {},
   "source": [
    "## Select the best model\n",
    "- so here we have some list of the best text classification algorithms we imported. Now we will compare each model's score and see which model is performing better than rest of the others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a928453",
   "metadata": {},
   "source": [
    "### 1. Multinomial Naive Bayes Classifier\n",
    "\n",
    "The multinomial NB classifier has a hyperparameter called **`alpha`**. It is the **smoothing parameter** to avoid **zero counts** when calculating the frequencies. \n",
    "\n",
    "For example, if we are now classifying a new SMS with a word \"ryan\" which never exist in the spam emails within our training dataset, the **likelihood** for this word will be zero. This will casue the **overall likelihood** to be zero (because we take the product of all **individual likelihoods**) for no matter what class of output variable we have.\n",
    "\n",
    "Therefore, we need to add **additional counts** to each word when calculating the frequencies to avoid have a zero likelihood value. **Alpha** indicates how many **additional counts** we add."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739d885",
   "metadata": {},
   "source": [
    "### 2. Gaussian Naive Bayes Classifier\n",
    "\n",
    "There is one hyperparameter we need to tune: **`var_smoothing`**. This is the **portion of the largest variance** of all features that is added to variances for **calculation stability**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a83390",
   "metadata": {},
   "source": [
    "### 3. SVC\n",
    "SVC, or Support Vector Classifier, is a supervised machine learning algorithm typically used for classification tasks. SVC works by mapping data points to a high-dimensional space and then finding the optimal hyperplane that divides the data into two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28911581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "models = {\n",
    "    \"Multinomial Naive Bayes\": MultinomialNB(),\n",
    "    \"Gaussian Naive Bayes\": GaussianNB(),\n",
    "    \"SVC\": SVC()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9802e9",
   "metadata": {},
   "source": [
    "- ### We will create a generic function to check each model's performance so that we can compare those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3250ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function which can evaluate models and return a report \n",
    "def evaluate_models(X, y, models):\n",
    "    '''\n",
    "    This function takes in X and y and models dictionary as input\n",
    "    It splits the data into Train Test split\n",
    "    Iterates through the given model dictionary and evaluates the metrics\n",
    "    Returns: Dataframe which contains report of all models metrics with cost\n",
    "    '''\n",
    "    # separate dataset into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "    \n",
    "\n",
    "    models_list = []\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(len(list(models))):\n",
    "        model = list(models.values())[i]\n",
    "        model.fit(X_train, y_train) # Train model\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        score = accuracy_score(y_test,y_pred)\n",
    "        \n",
    "        model_name = list(models.keys())[i]\n",
    "        print(f'---- score for --- {model_name} ----')\n",
    "        print(f\"{score}\")\n",
    "        models_list.append(model_name)\n",
    "        scores.append(score)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    report = pd.DataFrame()\n",
    "    report['Model_name'] = models_list\n",
    "    report['Score'] = scores        \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5d0768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- score for --- Multinomial Naive Bayes ----\n",
      "0.9387186629526463\n",
      "---- score for --- Gaussian Naive Bayes ----\n",
      "0.9623955431754875\n",
      "---- score for --- SVC ----\n",
      "0.9303621169916435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = evaluate_models(x, y, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "441c6427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_name</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.930362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Multinomial Naive Bayes</td>\n",
       "      <td>0.938719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.962396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model_name     Score\n",
       "2                      SVC  0.930362\n",
       "0  Multinomial Naive Bayes  0.938719\n",
       "1     Gaussian Naive Bayes  0.962396"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.sort_values('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914f889",
   "metadata": {},
   "source": [
    "- ### From the report above we can see that the Gaussian Naive Bayes model performed the best, so we will continue training our model using Gaussian Naive Bayes algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23b0db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2686f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'var_smoothing': 1.0182959106004854e-09}\n",
      "accuracy : 0.9536887824235386\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "params = {'var_smoothing': np.random.exponential(.000000001,20)\n",
    "         }\n",
    "gnb_model = GaussianNB()\n",
    "gnb_cv = GridSearchCV(gnb_model, params, cv = 10)\n",
    "gnb_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",gnb_cv.best_params_)\n",
    "print(\"accuracy :\",gnb_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bd35d11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model is 0.9623955431754875\n",
      "The confusion matrix is: \n",
      "[[604   5]\n",
      " [ 22  87]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "spam_detect_model = GaussianNB(**gnb_cv.best_params_)\n",
    "spam_detect_model.fit(X_train, y_train)\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "confusion_m = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the model is {accuracy}\")\n",
    "print(f\"The confusion matrix is: \\n{confusion_m}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12489e",
   "metadata": {},
   "source": [
    "- So we can see that the model performed well and we have got an accuracy of 96% which is pretty insane. In our project we will be having all these models and we will be selecting the models based on the performence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
